---
title: "exercise2"
author: "Florian Beiser, Martin Lie"
date: "19.03.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster

In this exercise we analyse the coal-mining disasters in the UK from 1851 to 1962. 

## 1

The dataset contains the dates of coal-mining disasters with more than 10 victims. We start with an illustration of the data.

```{r A1}
# Load data
coal = as.double(as.matrix(read.table("coalmine.txt")))

# Plotting
plot(coal[-c(1,length(coal))], seq(length(coal)-2),
     main="Cumulative number of coal-mining disasters",
     xlab="time(years)", ylab="")
```

The previous figure suggests that between 1850 and 1890 disasters happend very regularily such that we have an almost linear increase in the cumulative number. After 1890 the frequency reduced, but it still seems rather regularily (somewhat linear behaviour with less steep slope).

For statistical modelling a hierachical Bayesian model with an inhomogenous Poisson process is chosen. The time interval is devided into 2 parts $[t_0,t_1]$ and $[t_1,t_2]$ where $t_0$ and $t_2$ are the start and end point of the observation interval respectively. On each interval the Poisson model follows the coefficients $\lambda_0$ and $\lambda_1$ respectively. For an observation $x=(y_1,y_2)$ were $y_i$ are the number of disasters in interval $i$, the likelihood is
$$
f(x|\theta) = \exp(-\lambda_0(t_1-t_0)-\lambda_1(t_2-t_1))\lambda_0^{y_0}\lambda_1^{y_1}.
$$
The Poisson coefficients are equipped with a $Gamma(2,\beta)$ prior:
$$
f(\lambda_i|\beta) = \frac1{\beta^2}\lambda_i\exp(\frac{\lambda_i}\beta)
$$

Last the hyperparameter $\beta$ get the improper prior 
$$
f(\beta)\propto \frac1\beta \exp(-\frac1\beta)
$$

For notational convenience all parameters are collected together $\theta=(t_1,\lambda_0, \lambda_1,\beta)$.

## 2

The posterior is calculated Bayesian formula:

$$
\begin{align*}
f(\theta|x)&\propto f(x|\theta)f(\lambda_0|\beta)f(\lambda_1|\beta)f(t_1)f(\beta) \\
&= \frac1{\beta^5}\lambda_0^{y_0+1}\lambda_1^{y_1+1}\exp(\lambda_0t_0 - \lambda_0t_1 -\frac{\lambda_0}\beta + \lambda_1t_1 - \frac{\lambda_1}\beta - \lambda_1t_2 - \frac1\beta)\mathrm{1}_{[t_0,t_2]}(t_1)
\end{align*}
$$

## 3

The full conditionals are derived from the posterior by omitting all multiplicative factors that do not depend on the parameter of interest (remember that $f(\theta_i|x,\theta_{-i})\propto f(\theta|x)$):

$$
f(t_1|x,\theta_{-t_1}) \propto \lambda_0^{y_0}\lambda_1^{y_1} \exp(- (\lambda_0-\lambda_1)t_1 )\mathrm{1}_{[t_0,t_2]}(t_1) \sim \texttt{"unnamed"}
$$
$$
f(\lambda_i|x,\theta_{-\lambda_i}) \propto \lambda_i^{y_i+1}\exp(-\lambda_i ( t_{i+1} -t_i +\frac{1}\beta)) \sim Gamma(y_i+2,t_{i+1}-t_i +\frac{1}\beta))
$$

$$
f(\beta|x,\theta_{-\beta}) \propto \beta^{-5}\exp( -\frac{(\lambda_0+\lambda_1+1)}\beta) \sim IG(4,\lambda_0+\lambda_1+1)
$$
## 4 

We apply a single-site MCMC to sample from the posterior distribution

- Update $\beta$ by a Gibbs step
- Update $\lambda_0$ by a Gibbs step
- Update $\lambda_1$ by a Gibbs step
- Propose a new $t^*$ using a normal distribution with variance $\sigma^2$ in a random walk MCMC

The acceptance probability for the random walk step is $\alpha = \min(1,\frac{f(t^*|x,\theta_{-t})}{f(t|x,\theta_{-t})})$ where we calculate the latter quotient in log-scale.

```{r A4}
library(invgamma)
# From data
t0 = coal[1]
t2 = coal[length(coal)]
x = coal[-c(1,length(coal))]

# MCMC
# generating a chain by using aboves single site scheme

# Arguments:
# N: number of steps (including burn-in)
# sigma: tuning parameter for the RW site

MCMC <- function(N, sigma){
  # Initialisation
  beta = 0.1 
  lambda0 = rgamma(1, shape=2, scale=beta)
  lambda1 = rgamma(1, shape=2, scale=beta)
  t1 = t0+(t2-t0)/2
  
  # Bookkeeping
  T1 = rep(t1,N)
  Beta = rep(beta,N)
  Lambda0 = rep(lambda0,N)
  Lambda1 = rep(lambda1,N)
  
  # MCMC Loop
  for (i in 2:N){
    # split x 
    y0 = length(x[x<t1])
    y1 = length(x)-y0
    
    # Update beta
    beta = rinvgamma(1, shape=4, lambda0+lambda1+1)
    Beta[i] = beta
    
    # Update lambda0
    lambda0 = rgamma(1, shape=y0+1, rate=-(t0-t1-1/beta))
    Lambda0[i] = lambda0
    
    # Update lambda1 
    lambda1 = rgamma(1, shape=y1+1, rate=-(t1-t2-1/beta))
    Lambda1[i] = lambda1
    
    # Propose t1
    t1_prop = min(max(t0,rnorm(1, mean=t1, sd=sigma)),t2)
    y0_prop = length(x[x<t1_prop])
    y1_prop = length(x)-y0_prop
    
    ldens_new = y0_prop*log(lambda0) + y1_prop*log(lambda1) -(lambda0-lambda1)*t1_prop
    ldens_old = y0*log(lambda0) + y1*log(lambda1) -(lambda0-lambda1)*t1

    # Accetance or rejection of beta proposal
    alpha = min(1,exp(ldens_new-ldens_old))
    if (runif(1) < alpha) {
      t1 = t1_prop
    } 
    T1[i] = t1
  }
  
  return(list(t1=T1,lambda0=Lambda0,lambda1=Lambda1,beta=Beta))
}
  

```


## 5 

We now build Markov chains using the single-site algorithm from above and evaluate its performance. For the length of the burn-in period we inspect trace plots and for the mixing we observe the autocorrelation (there are further criteria like ESS and Geweke diagnostics, but we focus on the previously mentioned, since they are very visual).

```{r}
# Input 
sigma = 3.5
N = 2000

# Extract chains from function
chains = MCMC(N, sigma)

# Arguments:
# chains: as output of MCMC function

# Function:
# generating traceplots for all parameters
tracePlots <- function(chains){
  par(mfrow=c(2,2))
  # Single chains
  Beta = chains$beta
  Lambda0 = chains$lambda0
  Lambda1 = chains$lambda1
  T1 = chains$t1
  
  plot(Beta, type="l", 
      main="Traceplot for beta", xlab="beta")
  plot(Lambda0, type="l",
       main="Traceplot for lambda0", xlab="lambda0")
  plot(Lambda1, type="l",
       main="Traceplot for lambda1", xlab="lambda1")
  plot(T1, type="l",
       main="Traceplot for t1", xlab="t1")
}

tracePlots(chains)

# Arguments:
# chains as by MCMC fucntion

# Function:
# Generating autocorrelation
acfPlots <- function(chains){
  par(mfrow=c(2,2))
  par(oma=c(2,0,2,0))  
  # Single chains
  Beta = chains$beta
  Lambda0 = chains$lambda0
  Lambda1 = chains$lambda1
  T1 = chains$t1
  
  acf(Beta, main="Autocorrelation for beta")
  acf(Lambda0, main="Autocorrelation for lambda0")
  acf(Lambda1, main="Autocorrelation for lambda0")
  acf(T1, main="Autocorrelation for t1")
}

acfPlots(chains)

# Reasonability of solution
t1_mean = mean(chains$t1[-c(1:100)])
x1 = x[x<t1_mean]
x2 = x[x>=t1_mean]
l0 = length(x1)/(x1[length(x1)]-x1[1])
l1 = length(x2)/(x2[length(x2)]-x2[1])

print(paste("From the MCMC we assume a split at ",t1_mean))
print(paste("From the data we estimate lambda0 for aboves t1 as ", l0))
print(paste("From the data we estimate lambda1 for aboves t1 as ", l1))
```

The trace plots suggest that the samples represent the target distribution after around 100 steps rather well and we cannot observe further changes of the distribution such that we choose the burn-in period of 100 steps. Further, the autocorrelation plots prove that the chains for $\beta,\lambda_0,\lambda_1$ mix very well as we expect it for Gibbs steps. For $t_1$ the mixing is not as good, since several steps get rejected and the chain pauses in a state for several steps.

Moreover, the distribution make a lot of sense to us since we already guessed that $t_1$ would be around 1890 (see sub-part 1) and in the Bayesian setting the distribution for $t_1$ which was generated with MCMC coincides with this intuition. Ad parameter, we can estimate the disasters per year before and after the cut at $t_1$, i.e. $\lambda_0$ and $\lambda_1$ from the data. For the parameter in the first interval we estimate around 3 disasters per year and this is again remodelled in the Bayesian setting of the MCMC with $\lambda_0$ having the mass of its distribution little above 3. Similarly, we observe a bit less than 1 disaster per year and $\lambda_1$ has it mass a bit below 1, such that this again coincides and we value our results as very reasonable. 



