---
title: "exercise3"
author: "Elsie Backen Tandberg, Maja Mathiassen, Florian Beiser"
date: "23.04.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: Comparing AR(2) parameter estimators using resampling of residuals

Given a non-Gaussian time-series $\mathrm{x}=(x_t)_{t=1}^T$ of length $T=100$, an AR(2) model is fitted to the data via least square and least absolute residuals, respectively, yielding two estimated parameters $\beta_1,\beta_2$ each. 

The AR(2) model is specified by the relatrion 
\begin{equation*}
x_t = \beta_1 x_{t-1} + \beta_2 x_{t-2} + e_t
\end{equation*}
where $e_t$ is some iid noice with zero mean and constant variance.

The least sum of squared residual method (LS) estimates the two parameters of the AR(2) model (note that the parameters are independent of time) using the loss function 
\begin{equation*}
Q_{LS}(\beta_1, \beta_2|x) = \sum_{t=3}^T(x_t - \beta_1x_{t-1} - \beta_2 x_{t-2})^2
\end{equation*}
and minimizing $Q_{LS}$ with resprect to $\beta_1, \beta_2$, resulting in a parameter estimate $\hat{\beta}_{LS}(\mathrm{x})=(\hat{\beta}_{1,LS},\hat{\beta}_{2,LS})$. 
In contrast, the least sum of absolute residual method (LA) using the loss function 
\begin{equation*}
Q_{LA}(\beta_1, \beta_2|x) = \sum_{t=3}^T|x_t - \beta_1x_{t-1} - \beta_2 x_{t-2}|
\end{equation*}
and the minimizing arguments of $Q_{LA}$ are denoted as the parameter estimate $\hat{\beta}_{LA}(\mathrm{x})=(\hat{\beta}_{1,LA},\hat{\beta}_{2,LS})$. 

The respective estimated residuals $\hat{e}=(x_t -\hat{\beta}_1 x_{t-1} - \hat{\beta}_2 x_{t-2}) _{t=3}^T$ are re-centered. (Note that there are no residuals for the first 2 time points.)

```{r 3Asources}
# The code requires that the extra files are downloaded from the course webpage 
# and stored in the same folder as this Rmd
source("probAhelp.R")
source("probAdata.R")

# Observed timeseries data
x = data3A$x

# Parameter estimates via LS and LA
betas = ARp.beta.est(x, 2)
beta_LS = betas$LS
beta_LA = betas$LA

# Re-centered residuals for LS and LA
e_LS = ARp.resid(x, beta_LS)
e_LA = ARp.resid(x, beta_LA)

```


## 1. Residual resampling bootstrap method

We use the residual resampling bootstrap method with $B=1500$ bootstrap samples to evaluate the performance of the LS in comparison to the LA estimator. Those samples utilize pseudo-time series which start at a random but consecutive time pair of the observed time series and is then constructed using the AR(2) model with resampled residuals. The bootstrap samples are the generated by estimating the parameters $\beta_1, \beta_2$ with LS and LA, respectively.

Based on those bootstrap samples the variances and biases of the LS and LA method are estimated, respectively. 

Using the bootstrap samples $(\mathrm{x}^b)_{b=1}^B$, the variance is then estimated by $\hat{Var}(\hat{\beta}) = \frac1{B-1}\sum_{b=1}^B(\hat{\beta}(\mathrm{x}^b)-\bar{\beta})$ where $\bar{\beta}=\frac1B\sum_{b=1}^B\hat{\beta}(\mathrm{x}^b)$ is the empirical mean. Note that $\hat{\beta}$ is the "true" value for the empirical distribution function of the observed time series, wherewith the bias is calculated as $\hat{bias}(\hat{\beta})=\bar{\beta}-\hat{\beta}$. We take these formulas for each of the two components of $\beta$ separately. 

```{r 3Abootstrap}
set.seed(0421)
# Input
B = 1500 # number of bootstrap samples
T = 100  # length of time series
p = 2

# Bookkeeping
bootstrap_betas_LS = matrix(0, ncol=p, nrow=B)
bootstrap_betas_LA = matrix(0, ncol=p, nrow=B)

# Bootstrapping for LS and LA in one (using same realisations)
for (b in 1:B){
  # Construct random but consecutive inital values
  idx0 = sample(T-p+1, 1)
  x0 = x[idx0:(idx0+p-1)]
  # Resample residuals
  idxs = sample(T-p, replace=TRUE)
  e_LS_b = e_LS[idxs]
  e_LA_b = e_LA[idxs]
  # pseudo response
  x_LS_b = ARp.filter(x0, beta_LS, e_LS_b)
  x_LA_b = ARp.filter(x0, beta_LA, e_LA_b)
  # bootstrap estimate
  beta_LS_b = ARp.beta.est(x_LS_b, p)$LS
  beta_LA_b = ARp.beta.est(x_LA_b, p)$LA
  # Storing beta estimate
  bootstrap_betas_LS[b,] = beta_LS_b
  bootstrap_betas_LA[b,] = beta_LA_b
}

# Estimate for variances
var_LS = apply(bootstrap_betas_LS,2,var)
print(paste("The variance for beta_1 in the LS estimator is ", var_LS[1]))
print(paste("The variance for beta_2 in the LS estimator is ", var_LS[2]))

var_LA = apply(bootstrap_betas_LA,2,var)
print(paste("The variance for beta_1 in the LA estimator is ", var_LA[1]))
print(paste("The variance for beta_2 in the LA estimator is ", var_LA[2]))

# Estimate for bias
bias_LS = colMeans(bootstrap_betas_LS) - beta_LS
print(paste("The bias for beta_1 in the LS estimator is ", bias_LS[1]))
print(paste("The bias for beta_2 in the LS estimator is ", bias_LS[2]))

bias_LA = colMeans(bootstrap_betas_LA) - beta_LA
print(paste("The bias for beta_1 in the LA estimator is ", bias_LA[1]))
print(paste("The bias for beta_2 in the LA estimator is ", bias_LA[2]))
```

For Gaussian AR(p) processes the LS estimator is optimal, but remember that the given process is non-Gaussian. Hereon the latest results, we observe that the LS estimator is no longer optimal for this problem since the LA estimator has smaller variance together with a smaller absolute bias! Hence, the LA is the preferable estimator for the given problem.


## 2: Predicion Interval based on bootstrapping

We use the previous bootstrap samples to build a prediction interval for $t=101$ for LS and LA, respectively. Therefore, we extend the observed timeseries $(x_t)_{t=1}^T$ utilizing the AR(2) by 
\begin{equation*}
x_{101}^b = \beta_1^b x_{100} + \beta_2^b x_{99} + e_{101}
\end{equation*}
where we plug in the previous bootstrap estimates for $\beta^b$ and draw $e_{101}$ from the residual distribution $\hat{e}_{LS}$ and $\hat{e}_{LA}$, respectively (this procedure got clarified with Sara in the exercise session). The prediction interval is finally calculated from the quantiles of this prediction sample set. 

```{r 3B}
# Bookkeeping
x101_LS = matrix(0, nrow=B)
x101_LA = matrix(0, nrow=B)

# Predicting x_101 for each bootstrap sample:
# x101 = b1*x100 + b2*x99 + e101
# where for b the estimates from part 1 are used
# and e101 is resampled from its empirical distribution
for (b in 1:B){
  x101_LS[b] = bootstrap_betas_LS[b,1]*x[100] + bootstrap_betas_LS[b,2]*x[99] + sample(e_LS,1)
  x101_LA[b] = bootstrap_betas_LA[b,1]*x[100] + bootstrap_betas_LA[b,2]*x[99] + sample(e_LA,1)
}

pred_interval_LS = quantile(x101_LS, probs=c(0.025,0.975))
pred_interval_LA = quantile(x101_LA, probs=c(0.025,0.975))

print(paste("The 95% prediction interval for LS is [",pred_interval_LS[1],",",pred_interval_LS[2],"]"))
print(paste("The 95% prediction interval for LA is [",pred_interval_LA[1],",",pred_interval_LA[2],"]"))
```

The 95% prediction intervall is for both estimators rather broad. The upper interval ends are both a bit above 23.0, but the lower end of LS is remarkably smaller (<7.5) than the one of LA (>10.0), such that the LA prediction intervall is a way smaller in the end. (Of course the estimate for the quantile values is quite dependent on the chosen seed due to a relatively small sample size for estimating a 2.5% or 97.5% quantile.)


