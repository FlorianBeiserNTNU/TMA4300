---
title: "exercise2"
author: "Florian Beiser, Martin Lie"
date: "19.03.2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem A: The coal-mining disaster

# Problem B: INLA for Gaussian data 

In this exercise we are going to consider the problem of smoothing for time series. 

The dataset `Gaussiandata.txt` is given. We read and visualise it.

```{r B0}
# Reading file and convert data type
y = as.double(as.matrix(read.table("Gaussiandata.txt")))
T = length(y)

# Visualise data points on a line
plot(seq(T),y,
     main="Data points from 'Gaussiandata.txt'",
     xlab="t", ylab="observation")
```


## The model

It is assumed that the data generation is described by a Gaussian likelihood:
$$1.\quad y_t|x_t\sim\mathcal{N}(x_t,1), \quad t=1,\dots,T$$
wherefrom it immediately follows that $\mathrm{y}|\mathrm{x}\sim\Pi\pi(y_t|x_t)$ which means that $\mathrm{y}$ is conditionally independent given $\mathrm{x}$. Note, taht here $y_t$ does not depend explicitly on $\theta$ but via $\mathrm{x}$ it still does. 
The latent field $\mathrm{x}$ which is modelled via a second order random walk with parameter $\theta$ such that
$$2.\quad  \pi(\mathrm{x}|\theta) \propto \mathcal{N}(\mathrm{0},\mathrm{Q}(\theta)^{-1})$$
is obviously a GMRF where we will go into detail of the precision matrix a bit further below.
Lastly, a Gamma prior is given for the hyperparameter $\theta$, i.e.
$$3. \quad \theta \sim Gamma(1,1).$$

In this exercise, we utilize different methods and implementation for Bayesian inference of the marginal posteriors - we will compare the results all together at the end and not after each subpart.

## 1.

Therewith, the model shows the clear structure of a latent Gaussian model, since the latent field is Gaussian distributed. Moreover, the precision matrix of the GMRF is sparse, which makes the model suitable for an INLA approach.

### About the precision matrix

In this paragraph, we show the form of the precision matrix which is derived from the RW2 ansatz. The part of interest in the exponent in the exponential, wherefore we only present those calculations here. In the second step those terms that allow to factor out $x_t$ are collected and mixed terms are split symmetrically.
$$
\begin{align*}
    \sum_{t=3}^T (x_t - 2x_{t-1} + x_{t-2})^2 & = \sum_{t=3}^T x_t^2 - 4x_t x_{t-1} + 2x_t x_{t-2} + 4x_{t-1}^2 - 4x_{t-1}x_{t-2} +x_{t-2} ^2\\
    &= x_1(x_1 - 2x_2 +x_3) + x_2(-2x_1 + 5x_2 -4x_3 +1 x_4) + \sum_{t=3}^{T-2} x_t(x_{t-2} -4 x_{t-1} + 6 x_t - 4 x_{t+1} + x_{t+2}) + \cdots \\
    &= \mathrm{x}^\top \mathrm{Q} \mathrm{x}
\end{align*}
$$

where 
$$
\mathrm{Q} = 
\begin{bmatrix} 
1 &-2 & 1 \\
-2 & 5 & -4 & 1\\
1 & -4 & 6 & -4 & 1 \\
& \ddots & \ddots & \ddots & \ddots & \ddots\\
& & 1 & -4 & 6 &-4 & 1 \\
& & & 1 & -4 & 5 &-2 \\
& & & & 1 & -2 & 1 
\end{bmatrix}.
$$
This precision matrix is obviously sparse with bandwidth = 2. For later convenience, we introduce $\mathrm{Q}(\theta)=\theta\mathrm{Q}$. 

```{r B1}
# loads
library(Matrix)

# Construct the precision matrix
diags = list(rep(6,T), rep(-4,T-1), rep(1,T-2))
Q = as.matrix(bandSparse(T, T, k=c(0:2), diag=diags, symm=T))
Q[1,1] = Q[T,T] = 1
Q[1,2] = Q[2,1] = Q[T,T-1] = Q[T-1,T] = -2
Q[2,2] = Q[T-1,T-1] = 5

print(Q)
```

## 2.

For the aforementioned model, the new interest lies in the joint posterior $\pi(x,\theta|y)$. By Bayes' formula it can be represented by
$$
\pi(x,\theta|y) \propto \pi(y|x,\theta)\pi(x|\theta)\pi(\theta) = \pi(y|x)\pi(x|\theta)\pi(\theta)
$$
where the likelihood $\pi(y|x,\theta)$, the latent Gaussian field $\pi(x|\theta)$ and the hyper-prior $\pi(\theta)$ are given from the model, such that we obtain
$$
\pi(x,\theta|y) \propto \exp(-\frac12 (\mathrm{y}-\mathrm{x})^\top(\mathrm{y}-\mathrm{x})) \theta^{\frac{T-2}2}\exp(-\frac12 \mathrm{x}^\top\mathrm{Q(\theta)}\mathrm{x})\exp(-\theta)
$$
which is not in closed form such that direct sampling possible would be possible. This exercise employs MCMC to generate samples from the posterior, in the particular case a blocked Gibbs sampler is used. The proposal steps are split into 

-   new $\theta$ (single value) proposed from its full-conditional $\pi(\theta|\mathrm{x},\mathrm{y}$
-   new $\mathrm{x}$ (vector) proposed from its full-conditional $\pi(\mathrm{x}|\theta,\mathrm{y})$.

The full-conditionals are derived from the joint posterior be omitting factors that do not depend on the marginal variable and look like:

$$
\bullet\quad \pi(\theta|\mathrm{x},\mathrm{y}) \propto \theta^{\frac{T-2}2}\exp(-(\frac12 \mathrm{x}^\top\mathrm{Q}\mathrm{x}+1)\theta) \propto Gamma(\frac{T}2,\frac12 \mathrm{x}^\top\mathrm{Q}\mathrm{x}+1)\\
\bullet\quad \pi(\mathrm{x}|\theta,\mathrm{y}) \propto \exp(-\frac12 (\mathrm{x}\mathrm{Q}(\theta)\mathrm{x}  -2\mathrm{y}^\top\mathrm{x} ) \propto \mathcal{N}(\mathrm{\mu},\mathrm{\Sigma})
$$
where the latter is the canoical form a Gaussian distribution. However, to be able to sample from it we have to find the covariance matrix $\mathrm{\Sigma}$ and mean vector $\mathrm{\mu}$, which are used in the form $\exp(-\frac12 (\mathrm{x}-\mathrm{\mu})^\top \mathrm{C}^{-1} (\mathrm{x}-\mathrm{\mu}))$. Note that we do not care about multiplicative constants that are independent of $x$. Then we find by expending the covariance-formulation and comparing to above's canoncical coefficients:

- $\mathrm{\Sigma} = (\mathrm{Q}(\theta)+\mathrm{I})^{-1}$
- $\mathrm{\mu} = \mathrm{\Sigma}\mathrm{y}$

The Gibbs sampler iteratively updates $\theta$ and $\mathrm{x}$ from their respective proposal densities - in particular, the acceptance probability is always 1 for Gibbs.

```{r B2}
# loads
library(mvtnorm)

# Input parameters:
T = length(y)
x = rep(0, T) # initial x
theta = 0.0 # initial theta
N = 11000 # steps in markov chain

# Bookkeping
X = matrix(0, nrow=N, ncol=length(x))
Theta = matrix(0, nrow=N, ncol=1)

#----------------------
# MCMC loop
for (i in 1:N){
  # Update theta
  theta = rgamma(1, shape=T/2, rate=0.5*t(x)%*%Q%*%x+1)
  Theta[i] = theta
  # Update x
  Sigma = solve(theta*Q+diag(T))
  mu = Sigma%*%y
  x = c(rmvnorm(1,mean=mu, sigma=Sigma))
  X[i,] = x
}

#----------------------
# Inference
burnin = 1000

# hyperparameter theta
hist(Theta[-c(1:burnin)], freq=F, breaks=50,
     main="Gibbs-MCMC posterior marginal for theta",
     xlab="theta")

# latent field x_10
hist(X[-c(1:burnin),10], freq=F, breaks=50,
     main="Gibbs-MCMC posterior marginal for x_10",
     xlab="x_10")

# latent field x
Xmeans = colMeans(X[-c(1:burnin),])
Xquants = apply(X[-c(1:burnin),], 2, quantile, probs=c(0.025,0.975))

Ts = seq(T)
plot(Ts,y,
     main="Gibbs-MCMC inference for the latent field",
     xlab="t", ylab="value")
lines(Ts, Xmeans)
lines(Ts, Xquants[1,], lty=2)
lines(Ts, Xquants[2,], lty=2)

```
The histogram shows the posterior marginal for $\theta$ which is obtained by Gibbs MCMC. The plot shows again the observation data together with the linearly interpolated posterior means (solid line) and the 0.025 as well as 0.975 quantile (dashed line) from the Gibbs MCMC. We use 10.000 MCMC samples after a burn-in period of 1.000 steps - an inspection of the trace plots suggests that there is only a short burn-in phase. 


## 3. 

As second approach for inference with the posterior, the exercise constrasts the MCMC results with the INLA method. The method focuses on the posterior marginals (which we actually analyzed exclusively in the previous subsection). The INLA scheme relies on numerical approximation of the posterior marginals using the identity
$$
\pi(\theta|\mathrm{x}) \propto \frac{\pi(\mathrm{y}|\mathrm{x},\theta)\pi(\mathrm{x}|\theta)\pi(\theta)}{\pi(\mathrm{x}|\theta,\mathrm{y})}
$$
where it is noteworthy that all distributions are known from before:

- $\pi(\mathrm{y}|\mathrm{x},\theta)=\pi(\mathrm{y}|\mathrm{x})\sim\mathcal{N}(\mathrm{x},\mathrm{I})$
- $\pi(\mathrm{x}|\theta) \sim \mathcal{N}(0,\mathrm{Q}(\theta))$
- $\pi(\theta) \sim Gamma(1,1)$
- $\pi(\mathrm{x}|\theta,\mathrm{y})\sim\mathcal{N}(\mathrm{\mu},\mathrm{\Sigma})$

The approximation is build using a $\theta$-grid in the interval $[0,6]$, moreover this is valid for all $x$ wherefore we choose $\mathrm{x}=0$ for simplicity, such that several terms will vanish. 

- First, the hyperprior is independent of $x$, where $\pi(\theta)=\exp(-\theta)$
- For $\mathrm{x}=0$ in the latent Gaussian field the argument in the exponential vanishes and therefore it only contributes a constant factor of 1, such that $\pi(0|\theta)=\theta^{\frac{T-2}2}$
- The likelihood does not depend explicitly on $\theta$. Further, $\mathrm{x}=0$ means that the mean in the normal distribution is $0$ and we end up with the standard normal (since variances are 1 and uncorrelated) whose density value is evaluted at $\mathrm{y}$, i.e. $\pi(\mathrm{y}|0) \equiv \exp(-\frac12\mathrm{y}^\top\mathrm{y})$
- Lastly, the full conditional of the latent field is as derived above $\pi(\mathrm{x})|\mathrm{y},\theta) = \exp(-\frac12(\mathrm{x}-\mathrm{\mu}(\theta))\Sigma(\theta)^{-1}(\mathrm{x}-\mathrm{\mu}(\theta)))$

Since those calculation involve products and ratios of very small numbers, we perform those calculations in log-scale. Moreover, we normalize the result at the end.


```{r B3}
# theta grid 
tgrid_reso = 0.25
thetas = seq(from=0, to=6, by=tgrid_reso)

x = rep(0,T)

# pi(theta)
lnumerator1 = -thetas 

# pi(x|theta)
lnumerator2 = ((T-2)/2)*log(thetas) 

# pi(y|x)
lnumerator3 = rep(-1/2*t(y)%*%y, length(thetas))

# pi(x|theta, y)
denominator = rep(0, length(thetas))
for (j in 1:length(thetas)){
  Sigma = solve(thetas[j]*Q+diag(T))
  mu = Sigma%*%y
  denominator[j] = dmvnorm(x,mean=mu, sigma=Sigma)
}
ldenominator = log(denominator)

lpost_marginal_theta = lnumerator1 + lnumerator2 + lnumerator3 - ldenominator
post_marginal_theta = exp(lpost_marginal_theta)
post_marginal_theta = post_marginal_theta/(sum(post_marginal_theta)*tgrid_reso)

plot(thetas, post_marginal_theta,
     main="INLA inference for hyper posterior",
     xlab="theta", ylab="density",
     type="l")
```


## 4. 

Since $\pi(\mathrm{x}|\theta,\mathrm{y})$ is Gaussian also its component $\pi(x_10|\theta,\mathrm{y})$ is Gaussian with distribution $\mathcal{N}(\mu_10,\Sigma_{10,10})$.


```{r B4}
index=10

# x10 grid
xgrid_reso = 0.1
xs = seq(from=-2, to=2, by=xgrid_reso)

post_marginal_xs <- function(index, xs){
  # bookkeeping
  post_marginal_x = rep(0,length(xs))
  post_fullcond_x = matrix(0, nrow=length(thetas), ncol=length(xs))
  post_fullcond_x_weighted = matrix(0, nrow=length(thetas), ncol=length(xs))
  
  # Numerical intregration of integral on x grid
  for (k in 1:length(thetas)){
    # dependencies of theta
    Sigma = solve(thetas[k]*Q+diag(T))
    mu = Sigma%*%y
    
    # pi(x|theta,y)
    post_fullcond_x[k,] = dnorm(xs, mu[index], sqrt(Sigma[index,index]))
    # pi(x|theta,y) pi(theta|y) Delta theta
    post_fullcond_x_weighted[k,] = post_fullcond_x[k,] * post_marginal_theta[k] * tgrid_reso
    # Update
    post_marginal_x = post_marginal_x + post_fullcond_x_weighted[k,]
  }
  
  return(list(post_marginal_x, post_fullcond_x, post_fullcond_x_weighted))

}

# Evaluating function and storing separately
post_marginal_x_result = post_marginal_xs(index, xs)
post_marginal_x = post_marginal_x_result[[1]]
post_fullcond_x = post_marginal_x_result[[2]]
post_fullcond_x_weighted = post_marginal_x_result[[3]]

# plotting pi(x|theta,y) 
plot(x=xs,y=post_fullcond_x[1,], type="l",
     main="INLA posterior conditioned on theta (unweighted)",
     ylim=c(0,2), xlab="x_10", ylab="density")
for (i in 2:length(thetas)){
  lines(xs, post_fullcond_x[i,])
}

# plotting pi(x|theta,y) pi(theta|y) Delta theta
plot(x=xs,y=post_fullcond_x_weighted[1,], type="l",
     main="INLA posterior conditioned on theta (weighted)",
     ylim=c(0,2), xlab="x_10", ylab="density")
for (i in 2:length(thetas)){
  lines(xs, post_fullcond_x_weighted[i,])
}

# Plotting pi(x|y)
plot(xs, post_marginal_x, type="l",
     main="INLA posterior marginal for x_10",
     xlab="x_10", ylab="density")

```


## 5.

In addition to the previous self-implemented INLA routines, we utilize now the `R-INLA` package for exactly the same model.

```{r B5}
library(INLA)
data = list(y=y, t=seq(length(y)) )

# Define model
formula = y ~ f( t, 
                 model = "rw2", 
                 hyper = list(prec=list(prior="loggamma", param=c(1,1))),
                 cyclic=F,
                 constr=T
                 )

# Calculate results
result = inla(formula = formula,
              data = data,
              family = "gaussian",
              control.family = list(hyper=list(prec=list(initial=0, fixed=TRUE))),
              control.predictor = list(compute = TRUE, link = 1)
              )

result$summary.hyperpar

#--------------------
# Inference

# marginal hyperparam
INLAtheta = inla.smarginal(result$marginals.hyperpar[[1]])

plot(INLAtheta, type="l", col="red",
     main="INLA approximation for hyper marginal",
     xlab="theta", ylab="density",
     xlim=c(0,6))

# marginal x10
INLAx10 = result$marginals.linear.predictor$Predictor.10

plot(INLAx10, type="l", col="red",
     main="R-INLA posterior for x10",
     xlab="x10", xlim=c(-2,2), ylab="density")

# marginal latent field
INLAx = result$summary.linear.predictor

plot(Ts,y,
     main="INLA inference for the latent field",
     xlab="t", ylab="value")
lines(Ts, INLAx$mean, col="red")
lines(Ts, INLAx$`0.025quant`, lty=2, col="red")
lines(Ts, INLAx$`0.975quant`, lty=2, col="red")

```


## Comparison of results

To put the quality of all methods into a nutshell, we compare the properties of the resulting distributions. 

### Posterior marginal for $\theta$

We compare the result of Gibbs-MCMC, (user implemented) INLA and R-INLA. 

- Histrogram from Gibbs-MCMC 
- Black line from INLA
- Red line from R-INLA

```{r Comparison1}
# Plotting as above
hist(Theta[-c(1:burnin)], freq=F, breaks=50,
     main="Posterior marginal for theta",
     xlab="theta", xlim=c(0,6))
lines(thetas, post_marginal_theta)
lines(INLAtheta_marginal, col="red")
```
We see that all methods agree (with respect to the chosen precision of calculations).

### Posterior marginal for $x_10$

We compare the result of Gibbs-MCMC, (user implemented) INLA and R-INLA. 

- Histrogram from Gibbs-MCMC 
- Black line from INLA
- Red line from R-INLA

```{r Comparison2}
# Plotting as above
hist(X[-c(1:burnin),10], freq=F, breaks=50,
     main="Posterior marginal for x_10",
     xlab="x_10", xlim=c(-2,2))
lines(xs, post_marginal_x)
lines(INLAx10, col="red")
```

The line of R-INLA is shifted but we assume that we have picked the wrong marginal version, but it is hard (or even impossible) to figure out the definitions from the "documentation".

### Posterior marginal for $x$

We compare the result of Gibbs-MCMC, (user implemented) INLA and R-INLA. 

- Green from Gibbs-MCMC 
- Red line from R-INLA

```{r Comparison3}
# Plotting as above
plot(Ts,y,
     main="Inference for the latent field",
     xlab="t", ylab="value")
lines(Ts, Xmeans, col="blue")
lines(Ts, Xquants[1,], lty=2, col="blue")
lines(Ts, Xquants[2,], lty=2, col="blue")

lines(Ts, INLAx$mean, col="red")
lines(Ts, INLAx$`0.025quant`, lty=2, col="red")
lines(Ts, INLAx$`0.975quant`, lty=2, col="red")
```
 
Note that we have not included INLA in this comparison since we have only insepected the 10th index before but all indices would be needed here. Nevertheless, the Gibbs-MCMC and R-INLA results coincide very well here.

